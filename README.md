# Charleston Concierge RAG Chatbot

## Introduction

Charleston Concierge is a conversational AI application designed to provide information about businesses, events, and places in Charleston, South Carolina. It leverages a Retrieval-Augmented Generation (RAG) approach, combining structured database lookups with semantic vector search to provide relevant and context-aware responses to user queries. The goal is to act as a helpful "local guide" for users seeking information about Charleston.

## Methodology

The core methodology is Retrieval-Augmented Generation (RAG):

1.  **Retrieval:** When a user asks a question, the system first retrieves relevant information from two primary sources:
    *   **SQL Database:** A SQLite database (`charleston.db`) stores structured information about businesses (name, location, description, categories) and events (name, date, time, venue, description). Direct SQL queries are used to find exact matches or filter based on criteria like dates or categories.
    *   **Vector Database:** ChromaDB vector stores (`data/charleston_combined_db`, `data/charleston_db`, `data/charleston_events_db`) contain embeddings of business and event descriptions. Semantic search (similarity search) is performed using HuggingFace sentence transformers (`all-MiniLM-L6-v2`) to find information semantically similar to the user's query, even if keywords don't match exactly.
2.  **Augmentation:** The retrieved information (context) from both SQL and vector searches is compiled into a structured format.
3.  **Generation:** This combined context, along with the original user query, is fed into a Large Language Model (LLM) via a specific prompt template. The LLM (currently configured for Ollama with the Mistral model) generates a natural language response based on the provided context and instructions, aiming for a helpful, conversational tone and specific formatting.

This hybrid approach aims to balance the accuracy of structured data with the flexibility of semantic search.

## System Design

The system consists of the following components:

*   **Flask Web Application (`app.py`):** Provides the user interface (chat window) and the backend API for handling chat messages and serving data.
*   **User Interface (`templates/chat.html`, `static/`):** A simple HTML page with JavaScript to interact with the Flask backend API.
*   **Database Manager (`utils/database_manager.py`):** Handles all interactions with the SQLite database (`charleston.db`), including table creation, data insertion, and querying.
*   **Vector Stores (`data/` directories):** ChromaDB directories storing the vector embeddings for semantic search. These are generated by `load_data_to_vector_db.py`.
*   **SQL Database (`charleston.db`):** SQLite file storing structured business and event data. Generated by data scraping scripts (not included for end-user) or potentially manual input.
*   **LLM Integration (Langchain):** Uses `langchain` and `langchain-community` to manage the LLM (Ollama), embedding models (HuggingFace), prompt templates, and the RAG chain (`LLMChain`, `RetrievalQA`).
*   **Data Loading Script (`load_data_to_vector_db.py`):** (Admin-only) Script to process data from the SQL database and populate the ChromaDB vector stores.

## Implementation Details

*   **Context Generation (`get_context_for_chat` in `app.py`):** This function orchestrates the retrieval step. It first queries the SQL database for businesses and events matching the user's query. It specifically looks for time references ("today", "weekend", etc.) to filter events by date. If the SQL search yields results, it formats them. If no SQL results are found, it falls back to performing a similarity search on the vector database(s).
*   **Prompt Engineering:** The `chat_template` in `app.py` provides specific instructions to the LLM on how to behave (friendly guide) and how to format the response (numbered list, detail on the first item).
*   **Dependencies:** Key Python libraries include Flask, Langchain, Langchain-Community, Ollama, ChromaDB, Sentence-Transformers, and Pandas.

## Installation

1.  **Clone the Repository:**
    ```bash
    git clone https://github.com/ghank7/Charleston_concierge.git 
    cd Charleston_concierge
    ```

2.  **Set up Python Environment:**
    It's highly recommended to use a virtual environment:
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```

3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Install and Run Ollama:**
    *   Follow the instructions on [Ollama.ai](https://ollama.ai/) to download and install Ollama for your operating system.
    *   Pull the Mistral model (or another model if you prefer and update `app.py`):
        ```bash
        ollama pull mistral
        ```
    *   Ensure the Ollama application is running in the background.

## Usage

1.  **Prepare Databases (Required First Time):**
    *   **SQL Database (`charleston.db`):** 
        *   This application requires the `charleston.db` SQLite database file to be present in the root directory of the project.
        *   This file is **not** included directly in the repository.
        *   **Download the pre-built `charleston.db` file from:** [**PLACEHOLDER: Add your download link here, e.g., GitHub Release, Google Drive, etc.**]
        *   Place the downloaded `charleston.db` file in the main project folder.
    *   **Vector Database:** Once `charleston.db` is downloaded and placed in the root directory, run the data loading script to create the necessary vector stores in the `data/` directory:
        ```bash
        python load_data_to_vector_db.py
        ```
        This script reads from the downloaded `charleston.db` and creates/updates the ChromaDB stores used for semantic search.

2.  **Run the Application:**
    Make sure Ollama is running and you have activated your virtual environment.
    ```bash
    python app.py
    ```

3.  **Access the Chatbot:**
    Open your web browser and navigate to `http://127.0.0.1:5001` (or the port specified in `app.py`).

4.  **Interact:**
    Type your questions about Charleston businesses and events into the chat interface.

## Project Structure

```
Charleston_concierge/
├── .gitignore
├── app.py                    # Main Flask application logic and API endpoints
├── load_data_to_vector_db.py # Script to build ChromaDB from charleston.db
├── README.md                 # This file
├── requirements.txt          # Python dependencies
├── static/                   # CSS, JavaScript for the frontend
│   └── style.css
│   └── script.js 
├── templates/
│   └── chat.html             # HTML template for the chat interface
├── utils/
│   └── database_manager.py   # Class for interacting with the SQL database
└── data/                     # Directory for vector store data (ignored by git)
    └── ...                   # ChromaDB files/directories will be created here 
```



## Running the Application

To run the application, execute:

```
python app.py
```

Then open a web browser and navigate to:
http://127.0.0.1:5000

## Usage

1. Enter a natural language query in the search box (e.g., "restaurants with outdoor seating").
2. Click the "Search" button or press Enter.

## Challenges

The primary challenge with the scope of this system is the dataset. Right now, it's usability is extremely limited by the fact that the database is lackluster in the amount of businesses and the type and quality of those entries. Getting it to this point required building scrapers for individual websites that performed decent for some websites and were non-functional for others. With an improved dataset consisting of multiple sources and a review-location api such as Yelp Fusion or Google Places (which cost money), the agent's parameters could be further improved for much better results.

## Future improvements

As stated, the program itself would benefit greatly from an improved dataset that consists of places (from a review service api) and also improved scrapers specifically for social media posts and articles written about the region. If the dataset contained much larger and longer descriptions, reviews, articles, etc., The possibility for a fully functioning local tour guide that rivals even the largest knowledge hubs of the area becomes very real.

